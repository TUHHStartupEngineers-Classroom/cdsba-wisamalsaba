[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Assignment 2\n\n# Load data from the stored file\nloaded_data &lt;- readRDS(\"C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/random_vars.rds\")\nView(loaded_data)\n\n# Extracting variables for analysis\nselected_age &lt;- loaded_data$age\nselected_income &lt;- loaded_data$income\n\n# Computing the Mean Values\naverage_age &lt;- sum(selected_age) / length(selected_age)\naverage_income &lt;- sum(selected_income) / length(selected_income)\ncat(\"Average age:\", average_age, \"\\n\")\n\n#&gt; Average age: 33.471\n\ncat(\"Average income:\", average_income, \"\\n\")\n\n#&gt; Average income: 3510.731\n\n# Calculating the Variance\nmean_age &lt;- mean(selected_age)\nmean_income &lt;- mean(selected_income)\nvariance_age &lt;- sum((selected_age - mean_age)^2) / (length(selected_age) - 1)\nvariance_income &lt;- sum((selected_income - mean_income)^2) / (length(selected_income) - 1)\ncat(\"Variance of age:\", variance_age, \"\\n\")\n\n#&gt; Variance of age: 340.6078\n\ncat(\"Variance of income:\", variance_income, \"\\n\")\n\n#&gt; Variance of income: 8625646\n\n# Computing the Standard Deviations\nstd_deviation_age &lt;- sqrt(variance_age)\nstd_deviation_income &lt;- sqrt(variance_income)\ncat(\"Standard deviation of age:\", std_deviation_age, \"\\n\")\n\n#&gt; Standard deviation of age: 18.45556\n\ncat(\"Standard deviation of income:\", std_deviation_income, \"\\n\")\n\n#&gt; Standard deviation of income: 2936.945\n\n# Exploring the Standard Deviation Comparison\nprint(\"Directly comparing standard deviations may not yield informative insights, especially when variables have distinct units and scales, such as age and income.\")\n\n#&gt; [1] \"Directly comparing standard deviations may not yield informative insights, especially when variables have distinct units and scales, such as age and income.\"\n\n# Calculating the Covariance\ncov_value &lt;- sum((selected_age - mean_age) * (selected_income - mean_income)) / length(selected_age)\ncat(\"Covariance:\", cov_value, \"\\n\")\n\n#&gt; Covariance: 29670.45\n\n# Calculating the Correlation\ncorrelation_value &lt;- cov_value / (std_deviation_age * std_deviation_income)\ncat(\"Correlation:\", correlation_value, \"\\n\")\n\n#&gt; Correlation: 0.5473952\n\n# Clarifying the Covariance vs. Correlation\nprint(\"Correlation, on a standardized scale [-1, 1], facilitates intuitive comparison, indicating the strength and direction of the relationship.\")\n\n#&gt; [1] \"Correlation, on a standardized scale [-1, 1], facilitates intuitive comparison, indicating the strength and direction of the relationship.\"\n\n# Conditional Expected Values for Income\nsubset_age_below_18 &lt;- subset(loaded_data, age &lt;= 18)\nconditional_exp_income_below_18 &lt;- mean(subset_age_below_18$income)\ncat(\"Conditional Expected Income for age &lt;= 18:\", conditional_exp_income_below_18, \"\\n\")\n\n#&gt; Conditional Expected Income for age &lt;= 18: 389.6074\n\nsubset_age_between_18_65 &lt;- subset(loaded_data, age &gt;= 18 & age &lt; 65)\nconditional_exp_income_between_18_65 &lt;- mean(subset_age_between_18_65$income)\ncat(\"Conditional Expected Income for age in [18, 65):\", conditional_exp_income_between_18_65, \"\\n\")\n\n#&gt; Conditional Expected Income for age in [18, 65): 4685.734\n\nsubset_age_above_65 &lt;- subset(loaded_data, age &gt;= 65)\nconditional_exp_income_above_65 &lt;- mean(subset_age_above_65$income)\ncat(\"Conditional Expected Income for age &gt;= 65:\", conditional_exp_income_above_65, \"\\n\")\n\n#&gt; Conditional Expected Income for age &gt;= 65: 1777.237"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Assignment 4\n\n# Simulating a dataset for the correlation between storks and birth rate with a potential confounding variable (urbanization)\n\n# Generate sample data for a few regions\nset.seed(987)\nregions &lt;- c(\"Urbanville\", \"Suburbia\", \"Ruraltown\", \"Cityville\", \"Countryside\")\nstorks_count &lt;- rpois(length(regions), lambda = 50)\nbirth_rate &lt;- rnorm(length(regions), mean = 15, sd = 5)\nurbanization &lt;- c(80, 60, 30, 90, 20)  # Urbanization percentage for each region\n\n# Creating a dataframe\ndata &lt;- data.frame(Region = regions, StorksCount = storks_count, BirthRate = birth_rate, Urbanization = urbanization)\n\n# Loading the ggplot2 library\nlibrary(ggplot2)\n\n# Creating a scatter plot with size representing urbanization\nggplot(data, aes(x = StorksCount, y = BirthRate, size = Urbanization)) +\n  geom_point() +\n  labs(title = \"Spurious Correlation: Storks vs Birth Rate\",\n       x = \"Storks Count\",\n       y = \"Birth Rate\",\n       size = \"Urbanization (%)\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Assignment 9\n\n# Loading required libraries\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(estimatr)\nlibrary(AER)\n\n#&gt; Loading required package: car\n#&gt; Loading required package: carData\n#&gt; \n#&gt; Attaching package: 'car'\n#&gt; \n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     recode\n#&gt; \n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     some\n#&gt; \n#&gt; Loading required package: lmtest\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; \n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n#&gt; \n#&gt; Loading required package: sandwich\n#&gt; Loading required package: survival\n\n# Q1 - Defining and Visualizing the DAG\ndag_structure &lt;- dagify(\n  spentTime ~ featureUsed,\n  spentTime ~ Unobserved,\n  featureUsed ~ Unobserved,\n  featureUsed ~ encourgement,\n  exposure = \"featureUsed\",\n  latent = \"Unobserved\",\n  outcome = \"spentTime\",\n  coords = list(x = c(Unobserved = 1, featureUsed = 0, spentTime = 2, encourgement = -1),\n                y = c(Unobserved = 1, featureUsed = 0, spentTime = 0, encourgement = 0)),\n  labels = c(\n    \"spentTime\" = \"Time Spent on the App\",\n    \"featureUsed\" = \"Usage of New Feature\",\n    \"encourgement\" = \"User Encouragement for Feature Use\",\n    \"Unobserved\" = \"Undisclosed Factors\"\n  )\n)\nggdag(dag_structure, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n# Data Loading and Exploration\ndata_frame &lt;- readRDS('C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/rand_enc.rds')\nhead(data_frame)\n\n\n\n  \n\n\n# Q2 - Calculating Naive Estimate\nnaive_est &lt;- lm(time_spent ~ used_ftr, data = data_frame)\nsummary(naive_est)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = data_frame)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n# Q3 - Assessing Correlation Matrix and Observations\ncorrelation_matrix &lt;- cor(data_frame) %&gt;% round(2)\n\ncat(\"  ## The naive estimate (10.82269) surpasses the IV robust estimate using rand_enc (9.738175), suggesting an upward bias in the naive estimate. It implies an overestimation of used_ftr's impact on time_spent.\")\n\n#&gt;   ## The naive estimate (10.82269) surpasses the IV robust estimate using rand_enc (9.738175), suggesting an upward bias in the naive estimate. It implies an overestimation of used_ftr's impact on time_spent.\n\ncat(\"## A strong correlation exists between used_ftr and time_spent.\")\n\n#&gt; ## A strong correlation exists between used_ftr and time_spent.\n\ncat(\"   ## Assuming rand_enc as an instrumental variable appears reasonable, as it displays a weak correlation with the outcome (time_spent) and a stronger correlation with the treatment (used_ftr).\")\n\n#&gt;    ## Assuming rand_enc as an instrumental variable appears reasonable, as it displays a weak correlation with the outcome (time_spent) and a stronger correlation with the treatment (used_ftr).\n\ncat(\"## Though the correlation between the instrumental variable and the outcome isn't zero (potentially due to noise), it remains relatively low.\")\n\n#&gt; ## Though the correlation between the instrumental variable and the outcome isn't zero (potentially due to noise), it remains relatively low.\n\n# Q4 - Estimating with Instrumental Variables Using 2SLS with rand_enc and Robust Standard Errors\nmodel_iv_robust &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = data_frame)\nsummary(model_iv_robust)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = data_frame)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n## Hansen J Test\nresiduals_iv &lt;- residuals(model_iv_robust)\nfitted_values_iv &lt;- fitted(model_iv_robust)\nhansen_test_stat &lt;- sum(residuals_iv * fitted_values_iv)\np_value_hansen &lt;- 1 - pchisq(hansen_test_stat, df = 1)\n\ncat(\"Hansen J Test Statistic:\", hansen_test_stat, \"\\n\")\n\n#&gt; Hansen J Test Statistic: 0\n\ncat(\"P-value:\", p_value_hansen, \"\\n\")\n\n#&gt; P-value: 1\n\ncat(\"  ## A Hansen J test with a test statistic near 0 and a p-value close to 1 suggests that the instrument used in the model doesn't violate over-identifying restrictions. It indicates the validity of the instrument, implying no evidence of endogeneity or correlation with the error term.\")\n\n#&gt;   ## A Hansen J test with a test statistic near 0 and a p-value close to 1 suggests that the instrument used in the model doesn't violate over-identifying restrictions. It indicates the validity of the instrument, implying no evidence of endogeneity or correlation with the error term.\n\ncat(\"Naive Estimate:\", coef(naive_est)['used_ftr'], \"\\n\")\n\n#&gt; Naive Estimate: 10.82269\n\ncat(\"IV Robust Estimate (rand_enc):\", model_iv_robust$coefficients['used_ftr'], \"\\n\")\n\n#&gt; IV Robust Estimate (rand_enc): 9.738175\n\ncat(\"  ## Since the naive estimate (10.82269) exceeds the IV robust estimate using rand_enc (9.738175), the naive estimate probably overestimates the effect of used_ftr on time_spent.\")\n\n#&gt;   ## Since the naive estimate (10.82269) exceeds the IV robust estimate using rand_enc (9.738175), the naive estimate probably overestimates the effect of used_ftr on time_spent."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "Assignment 7\n\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(ggplot2)\nlibrary(MatchIt)\n\n# Load data\ncustomer_data &lt;- readRDS(\"C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/membership.rds\")\n\n# Explore the data and check relationships between variables\nsummary(customer_data)\n\n#&gt;       age             sex         pre_avg_purch         card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :-14.23   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.82   1st Qu.:0.0000  \n#&gt;  Median :38.80   Median :1.0000   Median : 70.15   Median :0.0000  \n#&gt;  Mean   :40.37   Mean   :0.5038   Mean   : 70.42   Mean   :0.4232  \n#&gt;  3rd Qu.:49.20   3rd Qu.:1.0000   3rd Qu.: 88.79   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.42   Max.   :1.0000  \n#&gt;    avg_purch     \n#&gt;  Min.   :-28.61  \n#&gt;  1st Qu.: 54.02  \n#&gt;  Median : 76.24  \n#&gt;  Mean   : 76.61  \n#&gt;  3rd Qu.: 98.54  \n#&gt;  Max.   :192.91\n\ncor(customer_data)\n\n#&gt;                      age          sex pre_avg_purch        card   avg_purch\n#&gt; age           1.00000000  0.012532675   0.517506430 0.105533628 0.448632638\n#&gt; sex           0.01253267  1.000000000  -0.001221386 0.008468092 0.002181853\n#&gt; pre_avg_purch 0.51750643 -0.001221386   1.000000000 0.192333327 0.855828507\n#&gt; card          0.10553363  0.008468092   0.192333327 1.000000000 0.382352233\n#&gt; avg_purch     0.44863264  0.002181853   0.855828507 0.382352233 1.000000000\n\n# Create DAG using dagitty\ncollider &lt;- dagitty('dag {\n  avg_purch &lt;- plus_membership\n  avg_purch &lt;- age\n  avg_purch &lt;- sex\n  avg_purch &lt;- pre_avg_purch\n}')\n\n# Plot DAG using ggdag\nggdag(collider) +\n  geom_dag_point() +\n  geom_dag_text(color = \"red\") +\n  geom_dag_edges(edge_color = \"green\")\n\n\n\n\n\n\n\n########################################################################################\n# Naive estimation \nmodel_naive &lt;- lm(avg_purch ~ card   , data = customer_data)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n######################################################################################\ncem &lt;- matchit(card ~ age + pre_avg_purch+sex,\n               data = customer_data, \n               method = 'cem', \n               estimand = 'ATE')\n# Covariate balance\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = customer_data, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt; sex                  0.5040        0.5040          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0064          0.1222\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\n# Use matched data\ndf_cem &lt;- match.data(cem)\n\n# (2) Estimation\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n##################################################################################################\n# (1) Matching\n# replace: one-to-one or one-to-many matching\nnn &lt;- matchit(card ~ age + pre_avg_purch+sex,\n              data = customer_data,\n              method = \"nearest\", # changed\n              distance = \"mahalanobis\", # changed\n              replace = T)\n\n# Covariate Balance\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = customer_data, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16\n\n##############################################################################\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch+sex,\n                  data = customer_data,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch + sex, family = binomial(link = \"logit\"), \n#&gt;     data = customer_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Add propensities to table\ndf_aug &lt;- customer_data %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\ndf_aug\n\n\n\n  \n\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\ndf_ipw\n\n\n\n  \n\n\n# Look at data with IPW scores\ndf_ipw %&gt;% \n  select(card, age, pre_avg_purch,sex, propensity, ipw)\n\n\n\n  \n\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch  ~ card ,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "Assignment 5\n\n# Loading Required Libraries\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\n# Creating a DAG to visualize potential causal relationships between sales, parking spots, and location\nsales_dag &lt;- dagify(\n  sales ~ parking_spots,\n  sales ~ location,\n  parking_spots ~ location, # where location is the confounder\n  labels = c(\n    \"sales\" = \"Sales\",\n    \"parking_spots\" = \"Parking \\n Spots\",\n    \"location\" = \"Location\"\n  )\n)\nggdag(sales_dag, use_labels = \"label\",text= FALSE)\n\n\n\n\n\n\n\n# Loading and examining customer satisfaction data\nsat_data &lt;- readRDS('C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/customer_sat.rds')\nhead(sat_data)\n\n\n\n  \n\n\n# Regression Analysis\nmodel1 &lt;- lm(satisfaction ~ follow_ups, data = sat_data)\nsummary(model1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = sat_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\nmodel2 &lt;- lm(satisfaction ~ follow_ups + subscription, data = sat_data)\nsummary(model2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = sat_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n# Coefficients Comparison\ncoef_comparison &lt;- data.frame(\n  Model = c(\"Model 1\", \"Model 2\"),\n  Intercept = c(coef(model1)[1], coef(model2)[1]),\n  FollowUps = c(coef(model1)[2], coef(model2)[2]),\n  PremiumPlus = c(0, coef(model2)[grep(\"subscriptionPremium\\\\+\", names(coef(model2)))]),\n  Elite = c(0, coef(model2)[grep(\"subscriptionElite\", names(coef(model2)))])\n)\nprint(coef_comparison)\n\n#&gt;                        Model Intercept FollowUps PremiumPlus Elite\n#&gt;                      Model 1  78.88605 -3.309302     0.00000     0\n#&gt; subscriptionPremium+ Model 2  26.76667  2.194444    18.07222     0\n\n# Data Visualization\nsimps_not_cond &lt;- ggplot(sat_data, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F) +\n  labs(title = \"Relationship: Follow-ups and Satisfaction\",\n       x = \"Follow-ups\",\n       y = \"Satisfaction\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\nsimps_cond &lt;- ggplot(sat_data, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F, size = 1) +\n  labs(title = \"Relationship by Subscription Level\",\n       x = \"Follow-ups\",\n       y = \"Satisfaction\",\n       color = \"Subscription\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\nsimps_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsimps_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Assignment 6\n\nlibrary(ggplot2)\n# Load customer data\nonline_data &lt;- readRDS(\"C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/abtest_online.rds\")\nonline_data\n\n\n\n  \n\n\n# Create plots to compare different metrics\ncompare_purchase_amount_plot &lt;- \n  ggplot(online_data, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = FALSE) +\n  labs(x = NULL, y = \"purchase_amount\", title = \"Difference in purchase amount\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\ncompare_previous_visit_plot &lt;- \n  ggplot(online_data, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = FALSE) +\n  labs(x = NULL, y = \"previous_visit\", title = \"Difference in previous visit\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\ncompare_mobile_device_plot &lt;- \n  ggplot(online_data, \n         aes(x = chatbot, \n             y = mobile_device, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = FALSE) +\n  labs(x = NULL, y = \"mobile_device\", title = \"Difference in mobile device\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\ncompare_purchase_plot &lt;- \n  ggplot(online_data, \n         aes(x = chatbot,\n             y = purchase, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = FALSE) +\n  labs(x = NULL, y = \"purchase\", title = \"Difference in purchase\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\n# Display comparison plots\ncompare_purchase_amount_plot\n\n\n\n\n\n\n\ncompare_previous_visit_plot\n\n\n\n\n\n\n\ncompare_mobile_device_plot\n\n\n\n\n\n\n\ncompare_purchase_plot\n\n\n\n\n\n\n\n# Linear regression models\nlm_purchase &lt;- lm(purchase ~ chatbot, data = online_data)\nsummary(lm_purchase)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot, data = online_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.4960 -0.3249 -0.2679  0.5040  0.7321 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.49597    0.02122  23.376  &lt; 2e-16 ***\n#&gt; chatbotTRUE -0.22811    0.02989  -7.633 5.36e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4725 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.05516,    Adjusted R-squared:  0.05421 \n#&gt; F-statistic: 58.26 on 1 and 998 DF,  p-value: 5.36e-14\n\nlm_purchase_amount &lt;- lm(purchase_amount ~ chatbot, data= online_data)"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Assignment 8\n\n# Load necessary libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(readr)\nlibrary(lmtest)\n\n#&gt; Loading required package: zoo\n\n\n#&gt; \n#&gt; Attaching package: 'zoo'\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n\n# Load the dataset\nhospital_data &lt;- readRDS('C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/hospdd.rds')\n\n# 1. Compute mean satisfaction for 'treated' and 'control' hospitals pre- and post-treatment manually\n\n# Define treatment-related thresholds\ntreatment_month_threshold &lt;- 3.0\ntreatment_hospital_threshold &lt;- 18\n\n# Convert 'Month' and 'Hospital' columns to numeric\nhospital_data$month &lt;- as.numeric(hospital_data$month)\nhospital_data$hospital &lt;- as.numeric(hospital_data$hospital)\n\n# Segregate data into 'treated' and 'control' groups\ntreated_hospitals &lt;- hospital_data %&gt;%\n  filter(hospital &lt;= 18 )\n\ncontrol_hospitals &lt;- hospital_data %&gt;%\n  filter(hospital &gt; 18 )\n\n# Compute mean difference in satisfaction between treatment and control groups BEFORE treatment\nbefore_control_mean &lt;- control_hospitals %&gt;% \n  filter(month &lt;= treatment_month_threshold) %&gt;%\n  summarise(mean_hospital_satisfaction = mean(satis)) %&gt;%\n  pull(mean_hospital_satisfaction)\n\nbefore_treatment_mean  &lt;- treated_hospitals %&gt;% \n  filter(month &lt;= treatment_month_threshold) %&gt;%\n  summarise(mean_hospital_satisfaction = mean(satis)) %&gt;%\n  pull(mean_hospital_satisfaction)\n\nmean_difference_before &lt;- before_treatment_mean - before_control_mean\n\n# Compute mean difference in satisfaction between treatment and control groups AFTER treatment\nafter_control_mean &lt;- control_hospitals %&gt;% \n  filter(month &gt; treatment_month_threshold) %&gt;%\n  summarise(mean_hospital_satisfaction = mean(satis)) %&gt;%\n  pull(mean_hospital_satisfaction)\n\nafter_treatment_mean &lt;- treated_hospitals %&gt;% \n  filter(month &gt; treatment_month_threshold) %&gt;%\n  summarise(mean_hospital_satisfaction = mean(satis)) %&gt;%\n  pull(mean_hospital_satisfaction)\n\nmean_difference_after &lt;- after_treatment_mean - after_control_mean\n\n# Calculate Difference-in-Differences\nmean_difference_diff &lt;- mean_difference_after - mean_difference_before\n\n# 2. Use linear regression to estimate effects with fixed effects for groups and time\n\n# Fit linear regression model\nlinear_model &lt;- lm(satis ~ procedure * as.factor(month) + as.factor(month) + as.factor(hospital), data = hospital_data)\n\n# Display regression results\nsummary(linear_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ procedure * as.factor(month) + as.factor(month) + \n#&gt;     as.factor(hospital), data = hospital_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1880 -0.4630  0.0024  0.4540  4.3181 \n#&gt; \n#&gt; Coefficients: (3 not defined because of singularities)\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                  3.1716566  0.0562244  56.411  &lt; 2e-16 ***\n#&gt; procedure                    0.8785070  0.0541087  16.236  &lt; 2e-16 ***\n#&gt; as.factor(month)2           -0.0096077  0.0292138  -0.329 0.742261    \n#&gt; as.factor(month)3            0.0219686  0.0292138   0.752 0.452080    \n#&gt; as.factor(month)4            0.0152441  0.0368748   0.413 0.679326    \n#&gt; as.factor(month)5           -0.0246564  0.0368748  -0.669 0.503740    \n#&gt; as.factor(month)6            0.0055796  0.0368748   0.151 0.879734    \n#&gt; as.factor(month)7           -0.0238856  0.0368748  -0.648 0.517169    \n#&gt; as.factor(hospital)2         0.4085664  0.0772468   5.289 1.26e-07 ***\n#&gt; as.factor(hospital)3         0.5336248  0.0793436   6.725 1.88e-11 ***\n#&gt; as.factor(hospital)4         0.2275102  0.0739460   3.077 0.002101 ** \n#&gt; as.factor(hospital)5        -0.1453529  0.0739460  -1.966 0.049375 *  \n#&gt; as.factor(hospital)6         0.4478634  0.0739460   6.057 1.46e-09 ***\n#&gt; as.factor(hospital)7         1.4044164  0.0714606  19.653  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8         0.0718758  0.0763236   0.942 0.346365    \n#&gt; as.factor(hospital)9        -1.5185150  0.0782498 -19.406  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10        1.6828446  0.0772468  21.785  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11        0.2209653  0.0763236   2.895 0.003801 ** \n#&gt; as.factor(hospital)12       -0.0953034  0.0782498  -1.218 0.223287    \n#&gt; as.factor(hospital)13        0.4955931  0.0754708   6.567 5.50e-11 ***\n#&gt; as.factor(hospital)14        0.2330426  0.0793436   2.937 0.003323 ** \n#&gt; as.factor(hospital)15       -0.1444935  0.0793436  -1.821 0.068631 .  \n#&gt; as.factor(hospital)16        1.4142680  0.0772468  18.308  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17        0.4235429  0.0805415   5.259 1.49e-07 ***\n#&gt; as.factor(hospital)18        0.1532761  0.0938225   1.634 0.102369    \n#&gt; as.factor(hospital)19       -0.7453017  0.0811676  -9.182  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20        0.0473874  0.0791192   0.599 0.549234    \n#&gt; as.factor(hospital)21        1.1943370  0.0836287  14.281  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22        0.7993153  0.0823390   9.708  &lt; 2e-16 ***\n#&gt; as.factor(hospital)23        0.7017202  0.0811676   8.645  &lt; 2e-16 ***\n#&gt; as.factor(hospital)24       -0.3081260  0.0866459  -3.556 0.000379 ***\n#&gt; as.factor(hospital)25        0.6464736  0.0927319   6.971 3.41e-12 ***\n#&gt; as.factor(hospital)26        0.2142471  0.0791192   2.708 0.006787 ** \n#&gt; as.factor(hospital)27       -0.3986544  0.0766156  -5.203 2.01e-07 ***\n#&gt; as.factor(hospital)28        0.7119953  0.0836287   8.514  &lt; 2e-16 ***\n#&gt; as.factor(hospital)29        0.2485512  0.0800987   3.103 0.001923 ** \n#&gt; as.factor(hospital)30       -0.1679220  0.0953700  -1.761 0.078324 .  \n#&gt; as.factor(hospital)31        0.5120848  0.0791192   6.472 1.03e-10 ***\n#&gt; as.factor(hospital)32       -0.3233456  0.0800987  -4.037 5.47e-05 ***\n#&gt; as.factor(hospital)33       -0.4539752  0.0791192  -5.738 9.97e-09 ***\n#&gt; as.factor(hospital)34       -0.0004123  0.0746103  -0.006 0.995591    \n#&gt; as.factor(hospital)35        0.3541110  0.0766156   4.622 3.87e-06 ***\n#&gt; as.factor(hospital)36        2.1381425  0.0773862  27.630  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37        0.1404036  0.0927319   1.514 0.130049    \n#&gt; as.factor(hospital)38       -0.0868060  0.0782181  -1.110 0.267124    \n#&gt; as.factor(hospital)39       -0.0234969  0.0823390  -0.285 0.775370    \n#&gt; as.factor(hospital)40        1.1215331  0.0782181  14.339  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41       -0.1497346  0.0766156  -1.954 0.050697 .  \n#&gt; as.factor(hospital)42        0.8811369  0.0850564  10.359  &lt; 2e-16 ***\n#&gt; as.factor(hospital)43       -0.7724325  0.0811676  -9.517  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44        0.0344120  0.0904396   0.380 0.703588    \n#&gt; as.factor(hospital)45       -0.2137495  0.0766156  -2.790 0.005286 ** \n#&gt; as.factor(hospital)46        0.0784915  0.0823390   0.953 0.340484    \n#&gt; procedure:as.factor(month)2         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)3         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)4 -0.0750732  0.0684427  -1.097 0.272731    \n#&gt; procedure:as.factor(month)5  0.0061613  0.0684427   0.090 0.928272    \n#&gt; procedure:as.factor(month)6 -0.0531645  0.0684427  -0.777 0.437317    \n#&gt; procedure:as.factor(month)7         NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7239 on 7312 degrees of freedom\n#&gt; Multiple R-squared:  0.5334, Adjusted R-squared:  0.5299 \n#&gt; F-statistic:   152 on 55 and 7312 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Assignment 1.1\n\n# Define the probabilities and complementary probabilities\n# the dash refers to the complementary probability\nP_S &lt;- 0.3\nP_T_S &lt;- 0.2\nP_T_S_dash &lt;- 0.6\nP_S_dash &lt;- 1 - P_S\nP_T_dash_S &lt;- 1 - P_T_S\nP_T_dash_S_dash &lt;- 1 - P_T_S_dash\n\n# Calculate the conditional probabilities\n# inter refers to intersection\nP_T_inter_S &lt;- P_S * P_T_S\nP_T_inter_S_dash &lt;- P_S_dash * P_T_S_dash\nP_T_dash_inter_S &lt;- P_S * P_T_dash_S\nP_T_dash_inter_S_dash &lt;- P_S_dash * P_T_dash_S_dash\n\n# Execute the results\nprint(P_T_inter_S)\n\n#&gt; [1] 0.06\n\nprint(P_T_inter_S_dash)\n\n#&gt; [1] 0.42\n\nprint(P_T_dash_inter_S)\n\n#&gt; [1] 0.24\n\nprint(P_T_dash_inter_S_dash)\n\n#&gt; [1] 0.28\n\n\n\n\nAssignment 1.2\n\n# Load all necessary libraries\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\n\n# Total Number of observations\nnum_observations &lt;- 1000\n\n# Create tibble to simulate user device usage\nuser_devices &lt;- tibble(\n  user_id = 1:num_observations,\n  smartphone = rbinom(num_observations, 1, 0.4),\n  tablet = ifelse(smartphone == 1, rbinom(num_observations, 1, 0.2), rbinom(num_observations, 1, 0.5)),\n  computer = ifelse(tablet == 1, rbinom(num_observations, 1, 0.1), rbinom(num_observations, 1, 0.3))\n)\n\n# Ensure at least one device is active for each user\nuser_devices &lt;- user_devices %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n# Display the first ten lines\nhead(user_devices, 10)\n\n\n\n  \n\n\n# Get column sums\ncolumn_sums &lt;- colSums(user_devices)\n\n# Extract sets for each device\nusers_with_smartphone &lt;- which(user_devices$smartphone == 1)\nusers_with_tablet &lt;- which(user_devices$tablet == 1)\nusers_with_computer &lt;- which(user_devices$computer == 1)\n\n# List of all device sets\ndevice_sets &lt;- list(users_with_smartphone, users_with_tablet, users_with_computer)\n\n# Plot Venn diagram to visualize device usage overlap\nggVennDiagram(device_sets, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customize appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"white\"),\n        strip.background = element_rect(\"white\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\n# Calculate intersection of all three device sets\nusers_with_all_devices &lt;- Reduce(intersect, device_sets)\npercentage_all_devices &lt;- length(users_with_all_devices) / num_observations * 100\ncat(\"Percentage of customers using all three devices:\", round(percentage_all_devices, 2), \"%\\n\")\n\n#&gt; Percentage of customers using all three devices: 1 %\n\n# Calculate union of sets for users with at least two devices\nusers_with_at_least_two &lt;- union(intersect(users_with_smartphone, users_with_tablet), union(intersect(users_with_smartphone, users_with_computer), \n                                       intersect(users_with_computer, users_with_tablet)))\npercentage_at_least_two_devices &lt;- length(users_with_at_least_two) / num_observations * 100\ncat(\"Percentage of customers using at least two devices:\", round(percentage_at_least_two_devices, 2), \"%\\n\")\n\n#&gt; Percentage of customers using at least two devices: 22 %\n\n# Calculate sets for users using only one device independently\nusers_with_only_smartphone &lt;- users_with_smartphone[!(users_with_smartphone %in% users_with_tablet) & \n                                                      !(users_with_smartphone %in% users_with_computer)]\nusers_with_only_tablet &lt;- users_with_tablet[!(users_with_tablet %in% users_with_smartphone) & \n                                              !(users_with_tablet %in% users_with_computer)]\nusers_with_only_computer &lt;- users_with_computer[!(users_with_computer %in% users_with_smartphone) & \n                                                  !(users_with_computer %in% users_with_tablet)]\npercentage_only_one_device &lt;- (length(users_with_only_smartphone) + length(users_with_only_tablet) + \n                                 length(users_with_only_computer)) / num_observations * 100\ncat(\"Percentage of customers using just one device:\", round(percentage_only_one_device, 2), \"%\\n\")\n\n#&gt; Percentage of customers using just one device: 78 %\n\n\n\n\nAssignment 1.3\n\n#Calculating the probabilities\nProb_A &lt;- 0.04\nProb_B_A &lt;- 0.97\nProb_B_A_Dash &lt;- 0.01\nProb_A_Dash &lt;- 1- Prob_A\nProb_B_Dash_A &lt;- 1 - Prob_B_A\nProb_B_Dash_A_Dash &lt;- 1 - Prob_B_A_Dash\nProb_B &lt;- (Prob_B_A*Prob_A)+(Prob_B_A_Dash*Prob_A_Dash)\nProb_A_Dash_given_B &lt;- (Prob_B_A_Dash * Prob_A_Dash)/Prob_B\nProb_A_given_B &lt;- (Prob_B_A * Prob_A)/Prob_B\n#print the probabilities\nprint(Prob_A_Dash_given_B)\n\n#&gt; [1] 0.1983471\n\nprint(Prob_A_given_B)\n\n#&gt; [1] 0.8016529"
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Assignment 3\n\n# Importing necessary libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(modelr)\n\n# Loading the dataset on car prices\ncar_dataset &lt;- readRDS(\"C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/car_prices.rds\")\ncar_dataset\n\n\n\n  \n\n\n# Determining the size of the dataset\ndata_dimensions &lt;- dim(car_dataset)\nnum_rows &lt;- data_dimensions[1]\nnum_columns &lt;- data_dimensions[2]\ncat(\"Total rows:\", num_rows, \"\\n\")\n\n#&gt; Total rows: 181\n\ncat(\"Total columns:\", num_columns, \"\\n\")\n\n#&gt; Total columns: 22\n\n# Extracting column names for each dataset in the list\ncolumn_names_list &lt;- lapply(car_dataset, colnames)\n\n# Reviewing and explaining data types' importance\ncar_width_type &lt;- typeof(head(car_dataset$carwidth, 1))\ncat(\"Data type of 'car_width':\", car_width_type, \"\\n\")\n\n#&gt; Data type of 'car_width': double\n\ncar_body_type &lt;- typeof(head(car_dataset$carbody, 1))\ncat(\"Data type of 'car_body':\", car_body_type, \"\\n\")\n\n#&gt; Data type of 'car_body': character\n\nprint(\"Two types exist: 'character' and 'double'. Numerical data types like 'double' are suitable for quantitative info, while 'character' types hold categorical or textual data.\")\n\n#&gt; [1] \"Two types exist: 'character' and 'double'. Numerical data types like 'double' are suitable for quantitative info, while 'character' types hold categorical or textual data.\"\n\n# Preparing data for regression analysis\ncar_dataset[] &lt;- lapply(car_dataset, as.numeric)\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\ncar_dataset %&gt;%\n  cor() %&gt;% \n  round(2) %&gt;% \n  Matrix::tril()\n\n#&gt; 22 x 22 Matrix of class \"dtrMatrix\"\n#&gt;                  aspiration doornumber carbody drivewheel enginelocation\n#&gt; aspiration             1.00          .       .          .              .\n#&gt; doornumber               NA       1.00       .          .              .\n#&gt; carbody                  NA         NA    1.00          .              .\n#&gt; drivewheel               NA         NA      NA       1.00              .\n#&gt; enginelocation           NA         NA      NA         NA           1.00\n#&gt; wheelbase                NA         NA      NA         NA             NA\n#&gt; carlength                NA         NA      NA         NA             NA\n#&gt; carwidth                 NA         NA      NA         NA             NA\n#&gt; carheight                NA         NA      NA         NA             NA\n#&gt; curbweight               NA         NA      NA         NA             NA\n#&gt; enginetype               NA         NA      NA         NA             NA\n#&gt; cylindernumber           NA         NA      NA         NA             NA\n#&gt; enginesize               NA         NA      NA         NA             NA\n#&gt; fuelsystem               NA         NA      NA         NA             NA\n#&gt; boreratio                NA         NA      NA         NA             NA\n#&gt; stroke                   NA         NA      NA         NA             NA\n#&gt; compressionratio         NA         NA      NA         NA             NA\n#&gt; horsepower               NA         NA      NA         NA             NA\n#&gt; peakrpm                  NA         NA      NA         NA             NA\n#&gt; citympg                  NA         NA      NA         NA             NA\n#&gt; highwaympg               NA         NA      NA         NA             NA\n#&gt; price                    NA         NA      NA         NA             NA\n#&gt;                  wheelbase carlength carwidth carheight curbweight enginetype\n#&gt; aspiration               .         .        .         .          .          .\n#&gt; doornumber               .         .        .         .          .          .\n#&gt; carbody                  .         .        .         .          .          .\n#&gt; drivewheel               .         .        .         .          .          .\n#&gt; enginelocation           .         .        .         .          .          .\n#&gt; wheelbase             1.00         .        .         .          .          .\n#&gt; carlength             0.86      1.00        .         .          .          .\n#&gt; carwidth              0.77      0.83     1.00         .          .          .\n#&gt; carheight             0.54      0.44     0.20      1.00          .          .\n#&gt; curbweight            0.74      0.87     0.85      0.21       1.00          .\n#&gt; enginetype              NA        NA       NA        NA         NA       1.00\n#&gt; cylindernumber          NA        NA       NA        NA         NA         NA\n#&gt; enginesize            0.55      0.68     0.74     -0.02       0.87         NA\n#&gt; fuelsystem              NA        NA       NA        NA         NA         NA\n#&gt; boreratio             0.46      0.60     0.55      0.14       0.64         NA\n#&gt; stroke                0.07      0.07     0.11     -0.15       0.10         NA\n#&gt; compressionratio     -0.26     -0.25    -0.25     -0.05      -0.31         NA\n#&gt; horsepower            0.40      0.60     0.70     -0.09       0.82         NA\n#&gt; peakrpm              -0.22     -0.19    -0.11     -0.15      -0.16         NA\n#&gt; citympg              -0.58     -0.78    -0.74     -0.15      -0.87         NA\n#&gt; highwaympg           -0.63     -0.79    -0.75     -0.18      -0.89         NA\n#&gt; price                 0.56      0.67     0.74      0.07       0.83         NA\n#&gt;                  cylindernumber enginesize fuelsystem boreratio stroke\n#&gt; aspiration                    .          .          .         .      .\n#&gt; doornumber                    .          .          .         .      .\n#&gt; carbody                       .          .          .         .      .\n#&gt; drivewheel                    .          .          .         .      .\n#&gt; enginelocation                .          .          .         .      .\n#&gt; wheelbase                     .          .          .         .      .\n#&gt; carlength                     .          .          .         .      .\n#&gt; carwidth                      .          .          .         .      .\n#&gt; carheight                     .          .          .         .      .\n#&gt; curbweight                    .          .          .         .      .\n#&gt; enginetype                    .          .          .         .      .\n#&gt; cylindernumber             1.00          .          .         .      .\n#&gt; enginesize                   NA       1.00          .         .      .\n#&gt; fuelsystem                   NA         NA       1.00         .      .\n#&gt; boreratio                    NA       0.58         NA      1.00      .\n#&gt; stroke                       NA       0.18         NA     -0.10   1.00\n#&gt; compressionratio             NA      -0.16         NA     -0.20  -0.30\n#&gt; horsepower                   NA       0.85         NA      0.59   0.11\n#&gt; peakrpm                      NA      -0.18         NA     -0.24   0.07\n#&gt; citympg                      NA      -0.74         NA     -0.62  -0.09\n#&gt; highwaympg                   NA      -0.76         NA     -0.61  -0.07\n#&gt; price                        NA       0.89         NA      0.55   0.03\n#&gt;                  compressionratio horsepower peakrpm citympg highwaympg price\n#&gt; aspiration                      .          .       .       .          .     .\n#&gt; doornumber                      .          .       .       .          .     .\n#&gt; carbody                         .          .       .       .          .     .\n#&gt; drivewheel                      .          .       .       .          .     .\n#&gt; enginelocation                  .          .       .       .          .     .\n#&gt; wheelbase                       .          .       .       .          .     .\n#&gt; carlength                       .          .       .       .          .     .\n#&gt; carwidth                        .          .       .       .          .     .\n#&gt; carheight                       .          .       .       .          .     .\n#&gt; curbweight                      .          .       .       .          .     .\n#&gt; enginetype                      .          .       .       .          .     .\n#&gt; cylindernumber                  .          .       .       .          .     .\n#&gt; enginesize                      .          .       .       .          .     .\n#&gt; fuelsystem                      .          .       .       .          .     .\n#&gt; boreratio                       .          .       .       .          .     .\n#&gt; stroke                          .          .       .       .          .     .\n#&gt; compressionratio             1.00          .       .       .          .     .\n#&gt; horsepower                  -0.22       1.00       .       .          .     .\n#&gt; peakrpm                      0.16       0.08    1.00       .          .     .\n#&gt; citympg                      0.44      -0.81    0.03    1.00          .     .\n#&gt; highwaympg                   0.45      -0.78    0.05    0.98       1.00     .\n#&gt; price                       -0.18       0.84   -0.02   -0.74      -0.74  1.00\n\n# Describing the 'enginesize' variable\nfirst_eng_size &lt;- head(car_dataset$enginesize, 1)\nmin_eng_size &lt;- min(car_dataset$enginesize)\nmax_eng_size &lt;- max(car_dataset$enginesize)\n\ncat(\"'Enginesize' data type:\", typeof(first_eng_size), \", Ranges between:\", min_eng_size, \" and \", max_eng_size, \"\\n\")\n\n#&gt; 'Enginesize' data type: double , Ranges between: 61  and  326\n\n# Elaborating on enginesize's influence on price\ncat(\"'Enginesize' impacts price by around 89%, positively correlating. As the engine size increases, price rises proportionally.\")\n\n#&gt; 'Enginesize' impacts price by around 89%, positively correlating. As the engine size increases, price rises proportionally.\n\n# Creating a new variable related to seat heating\ncar_dataset_with_heating &lt;- car_dataset %&gt;%\n  mutate(seat_heating_true = sample(c(TRUE, FALSE), size = nrow(car_dataset), replace = TRUE))\n\n# Inspecting seat heating distribution\ntable(car_dataset_with_heating$seat_heating_true)\n\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;    88    93\n\n# Running regression considering seat heating\nlinear_model_with_heating &lt;- lm(price ~ seat_heating_true, data = car_dataset_with_heating)\n\n# Displaying the summary of the linear model\nsummary(linear_model_with_heating)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ seat_heating_true, data = car_dataset_with_heating)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -8331  -5450  -3154   3109  31951 \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            13449.2      861.2  15.617   &lt;2e-16 ***\n#&gt; seat_heating_trueTRUE   -875.5     1201.4  -0.729    0.467    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8079 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.002958,   Adjusted R-squared:  -0.002612 \n#&gt; F-statistic: 0.5311 on 1 and 179 DF,  p-value: 0.4671\n\ncat(\"Assigning all values as 'TRUE' for seat heating can eliminate variability, leading to issues like collinearity problems and difficulties in coefficient estimation in regression analysis.\")\n\n#&gt; Assigning all values as 'TRUE' for seat heating can eliminate variability, leading to issues like collinearity problems and difficulties in coefficient estimation in regression analysis."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Assignment 10\n\n# Load necessary libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n# Read data for the current campaign\ndata_current &lt;- readRDS('C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/coupon.rds')\n\n# [1] Regression Discontinuity Design Sensitivity Analysis ----\n\n# Define cut-off\ncutoff_value &lt;- 60\n\n# Bandwidths\nbandwidth_original &lt;- cutoff_value + c(-5, 5)\nbandwidth_half &lt;- cutoff_value + c(-5, 5) / 2\nbandwidth_double &lt;- cutoff_value + c(-5, 5) * 2\n\n# Function to perform regression discontinuity design analysis\nrun_rdd_analysis &lt;- function(data, bandwidth) {\n  \n  data_below &lt;- data %&gt;% filter(days_since_last &gt;= bandwidth[1] & days_since_last &lt; cutoff_value)\n  data_above &lt;- data %&gt;% filter(days_since_last &gt;= cutoff_value & days_since_last &lt;= bandwidth[2])\n  data_combined &lt;- bind_rows(data_above, data_below)\n  \n  lm_bandwidth &lt;- lm(purchase_after ~ days_since_last_centered + coupon, data_combined)\n  \n  model_below &lt;- lm(purchase_after ~ days_since_last, data_below)\n  model_above &lt;- lm(purchase_after ~ days_since_last, data_above)\n  \n  y0 &lt;- predict(model_below, tibble(days_since_last = cutoff_value))\n  y1 &lt;- predict(model_above, tibble(days_since_last = cutoff_value))\n  \n  treatment_effect &lt;- y1 - y0\n  return(list(LATE = treatment_effect, Summary = summary(lm_bandwidth)))\n}\n\n# Run analysis with original bandwidth\nLATE_original &lt;- run_rdd_analysis(data_current, bandwidth_original)\n\n# Run analysis with half the bandwidth\nLATE_half_bandwidth &lt;- run_rdd_analysis(data_current, bandwidth_half)\n\n# Run analysis with double the bandwidth\nLATE_double_bandwidth &lt;- run_rdd_analysis(data_current, bandwidth_double)\n\n# Printing results\ncat(\"Original Bandwidth:\\n\")\n\n#&gt; Original Bandwidth:\n\ncat(\"LATE:\", LATE_original$LATE, \"\\n\")\n\n#&gt; LATE: 7.989037\n\nprint(LATE_original$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = data_combined)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.4966  -2.1312  -0.0949   2.0185  10.4159 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.4242     0.3965  28.813  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.3835     0.1259   3.046  0.00251 ** \n#&gt; couponTRUE                 7.9334     0.7087  11.194  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.186 on 320 degrees of freedom\n#&gt; Multiple R-squared:  0.7074, Adjusted R-squared:  0.7055 \n#&gt; F-statistic: 386.8 on 2 and 320 DF,  p-value: &lt; 2.2e-16\n\ncat(\"\\nHalf the Bandwidth:\\n\")\n\n#&gt; \n#&gt; Half the Bandwidth:\n\ncat(\"LATE:\", LATE_half_bandwidth$LATE, \"\\n\")\n\n#&gt; LATE: 7.362377\n\nprint(LATE_half_bandwidth$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = data_combined)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9680  -2.2013   0.1676   2.1516   8.2567 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.6612     0.5747  20.292  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.6883     0.3219   2.138   0.0339 *  \n#&gt; couponTRUE                 7.1679     1.0172   7.047 3.87e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.289 on 178 degrees of freedom\n#&gt; Multiple R-squared:  0.6622, Adjusted R-squared:  0.6584 \n#&gt; F-statistic: 174.5 on 2 and 178 DF,  p-value: &lt; 2.2e-16\n\ncat(\"\\nDouble the Bandwidth:\\n\")\n\n#&gt; \n#&gt; Double the Bandwidth:\n\ncat(\"LATE:\", LATE_double_bandwidth$LATE, \"\\n\")\n\n#&gt; LATE: 9.513531\n\nprint(LATE_double_bandwidth$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = data_combined)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16\n\n# Output explanations with altered variable names\ncat(\"   ## There is no change in the three bandwidth choices on the statistical effect (positive effect of \n      coupon variable on purchase_after.\")\n\n#&gt;    ## There is no change in the three bandwidth choices on the statistical effect (positive effect of \n#&gt;       coupon variable on purchase_after.\n\ncat(\"   ## The estimated LATE is slightly lower for the half bandwidth than the original bandwidth, \n      suggesting a more conservative estimate.\")\n\n#&gt;    ## The estimated LATE is slightly lower for the half bandwidth than the original bandwidth, \n#&gt;       suggesting a more conservative estimate.\n\ncat(\"   ## The estimated LATE is higher for the double bandwidth than the original bandwidth, suggesting a \n      potentially broader impact on individuals farther from the cutoff point.\")\n\n#&gt;    ## The estimated LATE is higher for the double bandwidth than the original bandwidth, suggesting a \n#&gt;       potentially broader impact on individuals farther from the cutoff point.\n\ncat(\"   ## Bandwidth choice influences treatment (coupon TRUE) effect estimation.\")\n\n#&gt;    ## Bandwidth choice influences treatment (coupon TRUE) effect estimation.\n\ncat(\"   ## Coefficients vary with bandwidth changes.\")\n\n#&gt;    ## Coefficients vary with bandwidth changes.\n\n# [2] Different Past Campaign ----\n\n# Read data for the different past campaign\ndata_shipping &lt;- readRDS('C:/Users/wisam/OneDrive/Desktop/Causal_Data_Science_Data/Data/shipping.rds')\n\nlibrary(rddensity)\nrdd_density &lt;- rddensity(data_shipping$purchase_amount, c = 30)\nsummary(rdd_density)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rdd_density): There are repeated observations.\n#&gt; Point estimates and standard errors have been adjusted. Use option\n#&gt; massPoints=FALSE to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\ncat(\"   ## The manipulation tests indicate significant evidence of manipulation around the cut-off point (c=30)\")\n\n#&gt;    ## The manipulation tests indicate significant evidence of manipulation around the cut-off point (c=30)\n\ncat(\"   ## P-values of the robust T-statistic are close to zero, suggesting that there are systematic changes in the observed\n      density of the purchase_amount variable near the cut-off.\")\n\n#&gt;    ## P-values of the robust T-statistic are close to zero, suggesting that there are systematic changes in the observed\n#&gt;       density of the purchase_amount variable near the cut-off.\n\ncat(\"   ## The order of estimation and bias is also different on each side, suggesting a lack of smoothness or continuity.\")\n\n#&gt;    ## The order of estimation and bias is also different on each side, suggesting a lack of smoothness or continuity.\n\ncat(\"   ## The p-values of binomial tests provide further evidence of non-random behavior around the cut-off.\")\n\n#&gt;    ## The p-values of binomial tests provide further evidence of non-random behavior around the cut-off.\n\ncat(\"   ## The purchase_amount variable, based on the results of manipulation testing, may not be appropriate as a running \n      variable for an RDD with a cut-off at 30€\")\n\n#&gt;    ## The purchase_amount variable, based on the results of manipulation testing, may not be appropriate as a running \n#&gt;       variable for an RDD with a cut-off at 30€\n\n# Plotting histogram for confirmation\nggplot(data_shipping, aes(x = purchase_amount)) +\n  geom_histogram(binwidth = 5, fill = \"black\", color = \"white\") +\n  geom_vline(xintercept = 30, color = \"blue\", linetype = \"dashed\") +\n  xlab(\"Purchase Amount (€)\") +\n  ylab(\"Number of Purchases\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/01_journal/09_iv.html#hansen-j-test",
    "href": "content/01_journal/09_iv.html#hansen-j-test",
    "title": "1 Analyzing Causal Effects Using Instrumental Variables",
    "section": "7.1 Hansen J Test",
    "text": "7.1 Hansen J Test"
  }
]